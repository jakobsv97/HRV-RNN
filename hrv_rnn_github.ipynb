{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3.9.7\n",
    "\n",
    "Keras 2.9.0\n",
    "\n",
    "First we import all libraries and packages we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import .fit (Garmin) handling\n",
    "from fitparse import FitFile\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "#import ipynb.fit_functions as ffun\n",
    "%run fit_functions.ipynb\n",
    "\n",
    "#Signal processing\n",
    "from scipy import signal\n",
    "from scipy.ndimage import label\n",
    "from scipy.stats import zscore\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import trapz\n",
    "\n",
    "#Itertools\n",
    "import itertools\n",
    "\n",
    "#Time\n",
    "import time\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "#KERAS AND TENSORFLOW\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1234)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#TIMESERIES\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "#GARBAGE COLLECTOR\n",
    "import gc\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function formats the input data correctly.\n",
    "- dataset: data set\n",
    "- sequence_length: number of back lagged values used for prediction\n",
    "- sampling_rate: rate of samples in sequence_length\n",
    "- n_forecast: number of points we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, sequence_length,sampling_rate,n_forecast):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-sequence_length*sampling_rate-n_forecast+1):\n",
    "        a = dataset[i:(i+sequence_length*sampling_rate):sampling_rate]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + sequence_length*sampling_rate-(sampling_rate-1):i + sequence_length*sampling_rate+n_forecast-(sampling_rate-1)])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions calculate the HRV metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(rr_manual):\n",
    "    #Interpolate\n",
    "    x = np.cumsum(rr_manual) #In seconds\n",
    "    f = interp1d(x, rr_manual*1000, kind='cubic') #In milliseconds\n",
    "\n",
    "    fs = 4.0 #Sample frequency\n",
    "    steps = 1 / fs\n",
    "\n",
    "    # now we can sample from interpolation function\n",
    "    xx = np.arange(x[0], np.max(x), steps)\n",
    "    rr_interpolated = f(xx)\n",
    "    return rr_interpolated\n",
    "\n",
    "def frequency_domain(rri, fs=4):\n",
    "    # Estimate the spectral density using Welch's method\n",
    "    fxx, pxx = signal.welch(x=rri, fs=fs)\n",
    "    \n",
    "    '''\n",
    "    Segement found frequencies in the bands \n",
    "     - Very Low Frequency (VLF): 0-0.04Hz \n",
    "     - Low Frequency (LF): 0.04-0.15Hz \n",
    "     - High Frequency (HF): 0.15-0.4Hz\n",
    "    '''\n",
    "    cond_vlf = (fxx >= 0) & (fxx < 0.04)\n",
    "    cond_lf = (fxx >= 0.04) & (fxx < 0.15)\n",
    "    cond_hf = (fxx >= 0.15) & (fxx < 0.4)\n",
    "    \n",
    "    # calculate power in each band by integrating the spectral density \n",
    "    vlf = trapz(pxx[cond_vlf], fxx[cond_vlf])\n",
    "    lf = trapz(pxx[cond_lf], fxx[cond_lf])\n",
    "    hf = trapz(pxx[cond_hf], fxx[cond_hf])\n",
    "    \n",
    "    # sum these up to get total power\n",
    "    total_power = vlf + lf + hf\n",
    "\n",
    "    return vlf, lf, hf, total_power\n",
    "\n",
    "def hrv_metric(rr):\n",
    "    rr = np.array([rr])\n",
    "    rr = rr.reshape(-1)\n",
    "\n",
    "    # RMSSD: take the square root of the mean square of the differences\n",
    "    rmssd = np.sqrt(np.mean(np.square(np.diff(rr*1000)))) #Need the data to be in milliseconds, therefore rr*1000\n",
    "\n",
    "    # SDNN\n",
    "    sdnn = np.std(rr*1000) #Need the data to be in milliseconds, therefore rr*1000\n",
    "\n",
    "    try:\n",
    "        rr_interpolated = interpolate(rr)\n",
    "\n",
    "        vlf, lf, hf, total_power = frequency_domain(rr_interpolated,4)\n",
    "    except:\n",
    "        vlf, lf, hf, total_power = 'error','error','error','error'\n",
    "        \n",
    "    return rmssd, sdnn, vlf, lf, hf, total_power\n",
    "    #return rmssd, sdnn\n",
    "\n",
    "def cubic_spline(rr,start_hole,pred_length):\n",
    "\n",
    "    time_rr = np.cumsum(rr)\n",
    "    \n",
    "    x1 = time_rr[start_hole-2]\n",
    "    x2 = time_rr[start_hole-1]\n",
    "    x3 = time_rr[start_hole+pred_length]\n",
    "    x4 = time_rr[start_hole+pred_length+1]\n",
    "    \n",
    "    y1 = rr[start_hole-2]\n",
    "    y2 = rr[start_hole-1]\n",
    "    y3 = rr[start_hole+pred_length]\n",
    "    y4 = rr[start_hole+pred_length]\n",
    "\n",
    "    x = np.array([x1,x2,x3,x4])\n",
    "    f = interp1d(x, [y1,y2,y3,y4], kind='cubic') #In milliseconds\n",
    "\n",
    "    # now we can sample from interpolation function\n",
    "    xx = np.linspace(x2, x3,pred_length+2)\n",
    "    rr_interpolated = f(xx)\n",
    "    return rr_interpolated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Physionet data, specifically the 5 minute uphill walking segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys_path = 'ADD PATH HERE'\n",
    "phys_files = ([f for f in listdir(phys_path) if isfile(join(phys_path, f)) \n",
    "                      and not f.startswith('.')]) #Include this to exclude .ds_store file\n",
    "phys_files = np.sort(phys_files)\n",
    "phys_files\n",
    "\n",
    "phys_data = []\n",
    "for part in phys_files:\n",
    "    res_df = pd.read_csv(phys_path + part,sep=\",\")\n",
    "    idx = res_df.loc[res_df['[Params]']=='[HRData]'].index[-1]\n",
    "    idx+=1\n",
    "    physionet = res_df.iloc[idx:-1].astype(int).values\n",
    "    physionet = (physionet.reshape(-1))/1000\n",
    "    phys_data.append(physionet)\n",
    "\n",
    "physio_data = []\n",
    "physio_data.append(phys_data[0][4822:5491]) \n",
    "physio_data.append(phys_data[1][3823:4661])\n",
    "physio_data.append(phys_data[2][3798:4533])\n",
    "physio_data.append(phys_data[3][2299:2997]) \n",
    "physio_data.append(phys_data[4][2634:3320]) \n",
    "physio_data.append(phys_data[6][1810:2485]) \n",
    "physio_data.append(phys_data[7][4075:4778]) \n",
    "physio_data.append(phys_data[8][2222:2886]) \n",
    "physio_data.append(phys_data[9][2543:3281]) \n",
    "physio_data.append(phys_data[10][2724:3482])\n",
    "physio_data.append(phys_data[11][2211:2890]) \n",
    "physio_data.append(phys_data[12][3918:4671]) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create custom loss functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUSTOM LOSS FUNCTION\n",
    "import keras.backend as K\n",
    "\n",
    "#RMSSD\n",
    "def rmssd_loss(y_true, y_pred):\n",
    "    true = K.sqrt(K.mean(K.square(y_true[1:,:]-y_true[:-1,:])))\n",
    "    pred = K.sqrt(K.mean(K.square(y_pred[1:,:]-y_pred[:-1,:])))\n",
    "    loss = K.sqrt(K.mean(K.square(true-pred)))\n",
    "    return loss\n",
    "\n",
    "#DTW\n",
    "class DtwLoss(keras.losses.Loss):\n",
    "    def __init__(self, batch_size: int = 32):\n",
    "        super(DtwLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        tmp = []\n",
    "        for item in range(self.batch_size):\n",
    "            s = y_true[item, :]\n",
    "            t = y_pred[item, :]\n",
    "            n, m = len(s), len(t)\n",
    "            dtw_matrix = []\n",
    "            for i in range(n + 1):\n",
    "                line = []\n",
    "                for j in range(m + 1):\n",
    "                    if i == 0 and j == 0:\n",
    "                        line.append(0)\n",
    "                    else:\n",
    "                        line.append(np.inf)\n",
    "                dtw_matrix.append(line)\n",
    "\n",
    "            for i in range(1, n + 1):\n",
    "                for j in range(1, m + 1):\n",
    "                    cost = tf.abs(s[i - 1] - t[j - 1])\n",
    "                    last_min = tf.reduce_min([dtw_matrix[i - 1][j], dtw_matrix[i][j - 1], dtw_matrix[i - 1][j - 1]])\n",
    "                    dtw_matrix[i][j] = tf.cast(cost, dtype=tf.float32) + tf.cast(last_min, dtype=tf.float32)\n",
    "\n",
    "            temp = []\n",
    "            for i in range(len(dtw_matrix)):\n",
    "                temp.append(tf.stack(dtw_matrix[i]))\n",
    "\n",
    "            tmp.append(tf.stack(temp)[n, m])\n",
    "        return tf.reduce_mean(tmp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set scope of hyperparameter search in WandB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login()\n",
    "\n",
    "#DEFINE SWEEP METHOD\n",
    "sweep_config = {\n",
    "    'method':'grid'\n",
    "}\n",
    "\n",
    "#DEFINE METRICS\n",
    "metric = {\n",
    "    'name':'val_loss',\n",
    "    'goal':'minimize'\n",
    "}\n",
    "\n",
    "#DEFINE PARAMETERS\n",
    "parameters_dict = {\n",
    "    'number_layers': {\n",
    "        'values':[2,4,6]\n",
    "    },\n",
    "    'units': {\n",
    "        'values': [128,512,1024]\n",
    "        },\n",
    "    'sequence_length': {\n",
    "        'values':[50]\n",
    "    },\n",
    "    'sampling_rate': {\n",
    "        'values':[1,2]\n",
    "    },\n",
    "    'dropout': {\n",
    "          'values': [0.0]\n",
    "        },\n",
    "    'batch_size':{\n",
    "        'values': [1]\n",
    "    },\n",
    "    'learning_rate':{\n",
    "        'values':[0.0015]\n",
    "    },\n",
    "    'epochs':{\n",
    "        'values':[100]\n",
    "    },\n",
    "    'train_samples':{\n",
    "        'values':[0.5]\n",
    "    },\n",
    "    'error_function':{\n",
    "        'values':['RMSE','DTW','RMSSD']\n",
    "    },\n",
    "    'differencing':{\n",
    "        'values':['YES','NO']\n",
    "    },\n",
    "    'network':{\n",
    "        'values':['GRU','LSTM']\n",
    "    },\n",
    "    'len_test':{\n",
    "        'values':[0.3]\n",
    "    },\n",
    "    'patience':{\n",
    "        'values':[10]\n",
    "    },\n",
    "    'bidir':{\n",
    "        'values':['YES','NO']\n",
    "    },\n",
    "    'n_forecast':{\n",
    "        'values':[10]\n",
    "    },\n",
    "    'hole_length':{\n",
    "        'values':[10]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "sweep_config['metric'] = metric\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the function that gets the predicted values and calculates all the HRV metrics and stores them in WandB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_org, test, scaler, model_name, config,run, rolling):\n",
    "\n",
    "    \n",
    "    #GET PARAMETERS\n",
    "    error_function = config['error_function']\n",
    "    train_samples = config['train_samples']\n",
    "    len_test = config['len_test']\n",
    "    val_samples = 1-train_samples-len_test\n",
    "    diff = config['differencing']\n",
    "    n_forecast = config['n_forecast']\n",
    "    hole_length = config['hole_length']\n",
    "    sampling_rate=config[\"sampling_rate\"]\n",
    "    sequence_length=config[\"sequence_length\"]\n",
    "    batch_size = int(config[\"batch_size\"]*len(data_org))\n",
    "\n",
    "\n",
    "    #LOAD MODEL\n",
    "    if error_function == 'RMSE':\n",
    "        best_model = keras.models.load_model('article2/' + model_name  + '.h5',compile=False) #Need compile=False for it to work\n",
    "\n",
    "    if error_function =='DTW':\n",
    "        best_model = keras.models.load_model('article2/' + model_name  + '.h5',custom_objects={'DtwLoss':DtwLoss}, compile=False) #Need compile=False for it to work\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #GET ORIGINAL DATA WITH IMPUTED VALUES\n",
    "    true_vals = test.copy()\n",
    "    final_pred = true_vals.copy()\n",
    "    start_hole = int(len(final_pred)/2)\n",
    "\n",
    "\n",
    "    #IF DIFFERENCE\n",
    "    if diff == 'YES':\n",
    "        final_pred = np.diff(final_pred)\n",
    "        start_hole = start_hole - 1 #Hole starts one before when differencing\n",
    "\n",
    "\n",
    "\n",
    "    ###ROLLING PREDICTION###\n",
    "    if rolling == 'YES':\n",
    "        for i in range(n_forecast):\n",
    "            \n",
    "            #SCALE INPUT DATA\n",
    "            input_data = final_pred.copy()\n",
    "            input_data = input_data.reshape(len(final_pred),1)\n",
    "            input_data = scaler.transform(input_data)\n",
    "            input_data = input_data.reshape(len(final_pred))\n",
    "            \n",
    "            #PREPARE PREDICTION\n",
    "            X_test,Y_test = create_dataset(input_data[:start_hole+i+1],sequence_length,sampling_rate,1)\n",
    "\n",
    "\n",
    "            #PREDICT\n",
    "            predictions = best_model.predict(X_test,verbose=0)\n",
    "\n",
    "            \n",
    "            #UNSCALE PREDICTION\n",
    "            predictions = scaler.inverse_transform(predictions)\n",
    "            predictions = predictions.reshape(len(predictions))\n",
    "            \n",
    "            #ADD PREDICTION TO final_pred\n",
    "            final_pred[start_hole + i] = predictions[-1].copy() #Last value of prediction is the value we are after, since prepare_prediction is fed only up to this value\n",
    "\n",
    "\n",
    "    ###MIX PREDICTION n_forecast MUST BE 2 HERE\n",
    "    elif rolling=='MIX':\n",
    "        for i in range(5):\n",
    "            input_data = final_pred.reshape(-1,1).copy()\n",
    "            input_data = scaler.transform(input_data)\n",
    "            input_data = input_data.reshape(-1)\n",
    "            X_test,Y_test = create_dataset(input_data[:start_hole+n_forecast + i*n_forecast],sequence_length,sampling_rate,n_forecast)\n",
    "\n",
    "\n",
    "\n",
    "            predictions = best_model.predict(X_test,verbose=0)\n",
    "            predictions = scaler.inverse_transform(predictions)\n",
    "            predictions = predictions.reshape(-1)\n",
    "\n",
    "\n",
    "            final_pred[start_hole+i*2:start_hole+n_forecast+i*n_forecast] = predictions[-n_forecast:].copy()\n",
    "\n",
    "    \n",
    "    \n",
    "    ###SINGLE-SHOT PREDICTION###\n",
    "    else:\n",
    "\n",
    "        input_data = final_pred.reshape(-1,1).copy()\n",
    "        input_data = scaler.transform(input_data)\n",
    "        input_data = input_data.reshape(-1)\n",
    "        X_test,Y_test = create_dataset(input_data[:start_hole+n_forecast],sequence_length,sampling_rate,n_forecast)\n",
    "\n",
    "\n",
    "        predictions = best_model.predict(X_test,verbose=0)\n",
    "        predictions = scaler.inverse_transform(predictions)\n",
    "        predictions = predictions.reshape(-1)\n",
    "\n",
    "\n",
    "        final_pred[start_hole:start_hole+n_forecast] = predictions[-n_forecast:]\n",
    "\n",
    "\n",
    "\n",
    "    #FINAL PREDICTIONS IF THE DATA WERE DIFFERENCED\n",
    "    if diff == 'YES':\n",
    "        start_hole +=1\n",
    "        preds = true_vals.copy()\n",
    "        preds[1:start_hole+hole_length] = true_vals[0].copy() + np.cumsum(final_pred[:start_hole-1+hole_length]) #preds will start the hole one index before final_pred, since final_pred is differenced and preds is not.\n",
    "        final_pred = preds.copy()\n",
    "\n",
    "    \n",
    "    #GET TRUE METRIC VALUES AND SAVE THEM IN WANDB\n",
    "    true_rmssd, true_sdnn, true_vlf, true_lf, true_hf, true_total_power = hrv_metric(true_vals)\n",
    "    wandb.run.summary['TRUE RMSSD'] = true_rmssd\n",
    "    wandb.run.summary['TRUE SDNN'] = true_sdnn\n",
    "    wandb.run.summary['TRUE VLF'] = true_vlf\n",
    "    wandb.run.summary['TRUE LF'] = true_lf\n",
    "    wandb.run.summary['TRUE HF'] = true_hf\n",
    "    wandb.run.summary['TRUE TOTAL POWER'] = true_total_power\n",
    "\n",
    "    #ROLLING PREDICTION SAVE IN WANDB\n",
    "    if rolling == 'YES':\n",
    "        #MAKE ARRAY OF PREDICTED VALUES\n",
    "        pred_lengths = [1,3,5,7,10]\n",
    "        for pred_length in pred_lengths:\n",
    "            \n",
    "            #GET ORIGINAL DATA WITH IMPUTED VALUES FOR VARIOUS HOLE SIZES (pred_length)\n",
    "            rolling_prediction = true_vals.copy()\n",
    "            rolling_prediction[:start_hole+pred_length] = final_pred[:start_hole+pred_length].copy()\n",
    "            \n",
    "            #GET METRICS\n",
    "            rmssd, sdnn, vlf, lf, hf, total_power = hrv_metric(rolling_prediction)\n",
    "        \n",
    "\n",
    "            #SAVE METRICS IN WANDB\n",
    "            hole_name = str(pred_length)\n",
    "            wandb.run.summary[hole_name + 'RMSSD'] = rmssd\n",
    "            wandb.run.summary[hole_name + 'SDNN'] = sdnn\n",
    "            wandb.run.summary[hole_name + 'VLF'] = vlf\n",
    "            wandb.run.summary[hole_name + 'LF'] = lf\n",
    "            wandb.run.summary[hole_name + 'HF'] = hf\n",
    "            wandb.run.summary[hole_name + 'TOTAL POWER'] = total_power\n",
    "\n",
    "            #SAVE ERROR IN METRICS IN WANDB\n",
    "            wandb.run.summary[hole_name + 'err_RMSSD'] = np.abs(rmssd-true_rmssd)/true_rmssd\n",
    "            wandb.run.summary[hole_name + 'err_SDNN'] = np.abs(sdnn-true_sdnn)/true_sdnn\n",
    "            wandb.run.summary[hole_name + 'err_VLF'] = np.abs(vlf-true_vlf)/true_vlf\n",
    "            wandb.run.summary[hole_name + 'err_LF'] = np.abs(lf-true_lf)/true_lf\n",
    "            wandb.run.summary[hole_name + 'err_HF'] = np.abs(hf-true_hf)/true_hf\n",
    "            wandb.run.summary[hole_name + 'err_TOTAL POWER'] = np.abs(total_power-true_total_power)/true_total_power\n",
    "\n",
    "            #CALCULATE CUBIC INTERPOLATION AND SAVE IN WANDB \n",
    "            rr_cubic = true_vals.copy()\n",
    "            cubic_inter = cubic_spline(rr_cubic,start_hole,pred_length)\n",
    "            rr_cubic[start_hole:start_hole+pred_length] = cubic_inter[1:-1].copy()\n",
    "            cub_rmssd, cub_sdnn, cub_vlf, cub_lf, cub_hf, cub_total_power = hrv_metric(rr_cubic)\n",
    "            \n",
    "            wandb.run.summary[hole_name + 'CUB RMSSD'] = cub_rmssd\n",
    "            wandb.run.summary[hole_name + 'CUB SDNN'] = cub_sdnn\n",
    "            wandb.run.summary[hole_name + 'CUB VLF'] = cub_vlf\n",
    "            wandb.run.summary[hole_name + 'CUB LF'] = cub_lf\n",
    "            wandb.run.summary[hole_name + 'CUB HF'] = cub_hf\n",
    "            wandb.run.summary[hole_name + 'CUB TOTAL POWER'] = cub_total_power\n",
    "            wandb.run.summary[hole_name + 'cub_err_RMSSD'] = np.abs(cub_rmssd-true_rmssd)/true_rmssd\n",
    "            wandb.run.summary[hole_name + 'cub_err_SDNN'] = np.abs(cub_sdnn-true_sdnn)/true_sdnn\n",
    "            wandb.run.summary[hole_name + 'cub_err_VLF'] = np.abs(cub_vlf-true_vlf)/true_vlf\n",
    "            wandb.run.summary[hole_name + 'cub_err_LF'] = np.abs(cub_lf-true_lf)/true_lf\n",
    "            wandb.run.summary[hole_name + 'cub_err_HF'] = np.abs(cub_hf-true_hf)/true_hf\n",
    "            wandb.run.summary[hole_name + 'cub_err_TOTAL POWER'] = np.abs(cub_total_power-true_total_power)/true_total_power\n",
    "\n",
    "        \n",
    "\n",
    "    else:\n",
    "        #GET PREDICTED VALUES FOR METRICS\n",
    "        rmssd, sdnn, vlf, lf, hf, total_power = hrv_metric(final_pred)\n",
    "        hole_name = str(hole_length)\n",
    "        wandb.run.summary[hole_name + 'RMSSD'] = rmssd\n",
    "        wandb.run.summary[hole_name + 'SDNN'] = sdnn\n",
    "        wandb.run.summary[hole_name + 'VLF'] = vlf\n",
    "        wandb.run.summary[hole_name + 'LF'] = lf\n",
    "        wandb.run.summary[hole_name + 'HF'] = hf\n",
    "        wandb.run.summary[hole_name + 'TOTAL POWER'] = total_power\n",
    "\n",
    "\n",
    "\n",
    "        wandb.run.summary[hole_name + 'err_RMSSD'] = np.abs(rmssd-true_rmssd)/true_rmssd\n",
    "        wandb.run.summary[hole_name + 'err_SDNN'] = np.abs(sdnn-true_sdnn)/true_sdnn\n",
    "        wandb.run.summary[hole_name + 'err_VLF'] = np.abs(vlf-true_vlf)/true_vlf\n",
    "        wandb.run.summary[hole_name + 'err_LF'] = np.abs(lf-true_lf)/true_lf\n",
    "        wandb.run.summary[hole_name + 'err_HF'] = np.abs(hf-true_hf)/true_hf\n",
    "        wandb.run.summary[hole_name + 'err_TOTAL POWER'] = np.abs(total_power-true_total_power)/true_total_power\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #CALCULATE CUBIC INTERPOLATION AND SAVE IN WANDB \n",
    "        rr_cubic = true_vals.copy()\n",
    "        cubic_inter = cubic_spline(rr_cubic,start_hole,hole_length)\n",
    "        rr_cubic[start_hole:start_hole+hole_length] = cubic_inter[1:-1].copy()\n",
    "        cub_rmssd, cub_sdnn, cub_vlf, cub_lf, cub_hf, cub_total_power = hrv_metric(rr_cubic)\n",
    "\n",
    "        \n",
    "        wandb.run.summary[hole_name + 'CUB RMSSD'] = cub_rmssd\n",
    "        wandb.run.summary[hole_name + 'CUB SDNN'] = cub_sdnn\n",
    "        wandb.run.summary[hole_name + 'CUB VLF'] = cub_vlf\n",
    "        wandb.run.summary[hole_name + 'CUB LF'] = cub_lf\n",
    "        wandb.run.summary[hole_name + 'CUB HF'] = cub_hf\n",
    "        wandb.run.summary[hole_name + 'CUB TOTAL POWER'] = cub_total_power\n",
    "        wandb.run.summary[hole_name + 'cub_err_RMSSD'] = np.abs(cub_rmssd-true_rmssd)/true_rmssd\n",
    "        wandb.run.summary[hole_name + 'cub_err_SDNN'] = np.abs(cub_sdnn-true_sdnn)/true_sdnn\n",
    "        wandb.run.summary[hole_name + 'cub_err_VLF'] = np.abs(cub_vlf-true_vlf)/true_vlf\n",
    "        wandb.run.summary[hole_name + 'cub_err_LF'] = np.abs(cub_lf-true_lf)/true_lf\n",
    "        wandb.run.summary[hole_name + 'cub_err_HF'] = np.abs(cub_hf-true_hf)/true_hf\n",
    "        wandb.run.summary[hole_name + 'cub_err_TOTAL POWER'] = np.abs(cub_total_power-true_total_power)/true_total_power\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    #SAVE ACTUAL AND PREDICTED VALUES AS WELL WHEN MIX CHANGE n_forecast TO 10\n",
    "    wandb.run.summary['pred_vals'] = np.round(final_pred[start_hole:start_hole+hole_length].copy(), decimals=3)\n",
    "    wandb.run.summary['true_vals'] = true_vals[start_hole:start_hole+hole_length].copy()\n",
    "    rmse = np.sqrt(np.mean((final_pred[start_hole:start_hole+hole_length]-true_vals[start_hole:start_hole+hole_length])**2))\n",
    "    wandb.run.summary['pred_RMSE'] = rmse\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model based on all the hyperparameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function():\n",
    "\n",
    "    #MARK DATA AS GLOBAL\n",
    "    global target_data\n",
    "    global person_nr\n",
    "\n",
    "    #INITIALIZE RUN\n",
    "    name = 'seq_' + 'person' + str(person_nr)\n",
    "    run = wandb.init(name=name)\n",
    "    config = wandb.config\n",
    "\n",
    "\n",
    "    data = target_data.copy()\n",
    "    data_org = target_data.copy()\n",
    "\n",
    "    #DEFINE SWEEP PARAMETERS\n",
    "    layers = config[\"number_layers\"]\n",
    "    units = config[\"units\"]\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    dropout = config[\"dropout\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "\n",
    "    train_samples = config[\"train_samples\"]\n",
    "    len_test = config['len_test']\n",
    "    val_samples = 1-train_samples-len_test\n",
    "    sampling_rate=config[\"sampling_rate\"]\n",
    "    sequence_length=config[\"sequence_length\"]\n",
    "    batch_size = int(config[\"batch_size\"]*len(data))\n",
    "    error_function = config[\"error_function\"]\n",
    "    diff = config[\"differencing\"]\n",
    "    network = config['network']\n",
    "    patience = config['patience']\n",
    "\n",
    "\n",
    "    #HOW MANY POINTS TO PREDICT\n",
    "    n_forecast = config['n_forecast']\n",
    "\n",
    "    #OPTIONS\n",
    "    bidir = config['bidir'] #Must be NO when rolling is YES\n",
    "    rolling = 'NO' # YES or NO or MIX\n",
    "\n",
    "    #GET SET SIZES\n",
    "    train_size = int(train_samples*len(data))\n",
    "    val_size = int(val_samples*len(data))\n",
    "    test = data[train_size+val_size:].copy()\n",
    "\n",
    "    #DIFFERENCE\n",
    "    if diff ==  'YES':\n",
    "        data = np.diff(data)\n",
    "\n",
    "\n",
    "    data = data.reshape(-1,1)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler = scaler.fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    data = data.reshape(-1)\n",
    "\n",
    "    #SPLIT DATA\n",
    "    train = data[:train_size].copy()\n",
    "    val = data[train_size:train_size+val_size].copy()\n",
    "\n",
    "\n",
    "\n",
    "    inputs = keras.Input(shape=(sequence_length,1)) #First value in shape is length of each series, and second value is number of features\n",
    "    x = inputs\n",
    "\n",
    "\n",
    "    if bidir == 'YES':\n",
    "        if network=='GRU':\n",
    "            for layer in range(layers):\n",
    "                x = keras.layers.Bidirectional(GRU(units=units,return_sequences=True))(x)\n",
    "                x = keras.layers.Dropout(dropout)(x)\n",
    "            x = keras.layers.Bidirectional(GRU(units=units,return_sequences=False))(x)\n",
    "        else:\n",
    "            for layer in range(layers):\n",
    "                x = keras.layers.Bidirectional(LSTM(units=units,return_sequences=True))(x)\n",
    "                x = keras.layers.Dropout(dropout)(x)\n",
    "            x = keras.layers.Bidirectional(LSTM(units=units,return_sequences=False))(x)\n",
    "\n",
    "    if bidir == 'NO':\n",
    "        if network=='GRU':\n",
    "            for layer in range(layers):\n",
    "                x = keras.layers.GRU(units=units,return_sequences=True)(x)\n",
    "                x = keras.layers.Dropout(dropout)(x)\n",
    "            x = keras.layers.GRU(units=units,return_sequences=False)(x)\n",
    "        else:\n",
    "            for layer in range(layers):\n",
    "                x = keras.layers.LSTM(units=units,return_sequences=True)(x)\n",
    "                x = keras.layers.Dropout(dropout)(x)\n",
    "            x = keras.layers.LSTM(units=units,return_sequences=False)(x)\n",
    "\n",
    "\n",
    "    x = keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    if rolling == 'YES':\n",
    "        X_train, Y_train = create_dataset(train,sequence_length,sampling_rate,1)\n",
    "        X_val, Y_val = create_dataset(val,sequence_length,sampling_rate,1)\n",
    "        #train_dataset, val_dataset, test_dataset = prepare_data(train_samples, val_samples, sampling_rate, sequence_length, batch_size, data)\n",
    "        print('We are rolling')\n",
    "        outputs = keras.layers.Dense(1)(x) #With rolling prediction we only want one output (last step)\n",
    "\n",
    "    else:\n",
    "        X_train, Y_train = create_dataset(train,sequence_length,sampling_rate,n_forecast)\n",
    "        X_val, Y_val = create_dataset(val,sequence_length,sampling_rate,n_forecast)\n",
    "        #train_dataset, val_dataset, test_dataset = prepare_data2(train_samples, val_samples, sampling_rate, sequence_length, batch_size, data, n_forecast)\n",
    "        print('Single-shot')\n",
    "        outputs = keras.layers.Dense(1*n_forecast)(x) #When using single-shot output (multistep output), we need as many outputs as points we want to predict. If several features we need to multiply by features (I think)\n",
    "\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience) #Stop when overfitting validation set, works well with 10. Stop when the validation loss is not improving after # epochs\n",
    "\n",
    "    model_name = 'model'\n",
    "    model_check = keras.callbacks.ModelCheckpoint('article2/' + model_name + '.h5',save_best_only=True, monitor=\"val_loss\") #Save best model based on \"monitor\"\n",
    "    callbacks = [model_check, early_stop,WandbCallback(save_model=False,save_graph=False)] #We dont need to save the models in wandb\n",
    "\n",
    "    \n",
    "    if error_function == 'RMSE':\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "\n",
    "    if error_function == 'DTW':\n",
    "        #Batch_size must be 1 with DTW\n",
    "        model.compile(optimizer=optimizer, loss=DtwLoss(batch_size=1))\n",
    "\n",
    "\n",
    "\n",
    "    model.fit(X_train,Y_train,epochs=epochs, validation_data=(X_val,Y_val),callbacks=callbacks, shuffle=False, verbose=0)\n",
    "\n",
    "    predict(data_org, test,scaler, model_name, config,run,rolling)\n",
    "\n",
    "\n",
    "    #FINISH RUN\n",
    "    run.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEFORE RUNNING:\n",
    "\n",
    "Make sure that rolling='NO' when using Bidirectional layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global person_nr\n",
    "person_nr = 0\n",
    "\n",
    "global target_data\n",
    "target_data = physio_data[person_nr].copy() #Person \n",
    "    \n",
    "sweep_id = wandb.sweep(sweep_config, project=\"PROJECT_NAME\")\n",
    "wandb.agent(sweep_id, function=train_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8158b3ce3d281913ae478ef4c1be93fea11e99d7cf4651b7388fb36667c146cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
